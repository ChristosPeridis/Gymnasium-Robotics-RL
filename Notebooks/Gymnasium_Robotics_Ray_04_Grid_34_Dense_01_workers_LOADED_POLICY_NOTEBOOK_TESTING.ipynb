{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14cc70ff-09db-477d-8fce-4ff963523d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "ray.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18ddbaf-bf7d-41e8-a54d-2dff0294b8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "resources = ray.cluster_resources()\n",
    "print(resources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4568b72-e993-494f-9b1b-084c505fde37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#env = gym.make('FrankaPickPlaceFixed-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355ad143-d043-4d2d-a824-f97890dea887",
   "metadata": {},
   "outputs": [],
   "source": [
    "def env_creator(env_config={}):\n",
    "    import gymnasium as gym\n",
    "    env = gym.make('FetchPickAndPlace-v2', render_mode='rgb_array', reward_type='dense', max_episode_steps=100)\n",
    "    #env = CustomWrapper(env)\n",
    "    env.reset()\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8bf628-1cad-48fe-b88a-1054b1bf2183",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.tune.registry import register_env\n",
    "register_env(\"Gymnasium_Pick_Place_0_Dense\", env_creator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f594feb-3a7c-4e0b-bbed-1e1e2d95e14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serialize with cloudpickle and save to a file\n",
    "#with open(\"Gymnasium_Pick_Place_0.pkl\", \"wb\") as f:\n",
    "   # cloudpickle.dump(env_creator, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c730fc0d-84e7-4a7a-9473-93d2988db9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421ff420-35f5-4a34-b7a8-99f0fece4129",
   "metadata": {},
   "outputs": [],
   "source": [
    "tune.run(\"PPO\",\n",
    "             # algorithm specific configuration\n",
    "             config={\"env\": \"Gymnasium_Pick_Place_0_Dense\",  #\n",
    "                     \"framework\": \"torch\",\n",
    "                     #\"seed\":958,\n",
    "                     \"num_gpus\": 1,\n",
    "                     \"num_rollout_workers\": 1,\n",
    "                     #\"num_envs_per_worker\": 1,\n",
    "                     #\"num_gpus_per_worker\": 0,\n",
    "                     \"train_batch_size\": 512,\n",
    "                     \"use_critic\":True,\n",
    "                     \"use_gae\": True,\n",
    "                     \"lambda\": 0.99,\n",
    "                     \"sgd_minibatch_size\": 32,\n",
    "                     \"num_sgd_iter\": 8,\n",
    "                     \"shuffle_sequences\": False,\n",
    "                     \"entropy_coeff\": 0.00015,\n",
    "                     \"clip_param\": 0.1,\n",
    "                     \"grad_clip\": 5,\n",
    "                     \"lr\": 0.00015,\n",
    "                     #\"num_cpus_per_worker\": 1,\n",
    "                     #\"model\": {\"custom_model\": \"pa_model\", },\n",
    "                     \"evaluation_interval\": 1,\n",
    "                     \"evaluation_num_episodes\": 2,\n",
    "                     \"evaluation_parallel_to_training\": True,\n",
    "                     \"evaluation_num_workers\": 1\n",
    "                     },\n",
    "             local_dir=\"../R3L-LOGS\",  # directory to save results\n",
    "             checkpoint_freq=2,  # frequency between checkpoints\n",
    "             keep_checkpoints_num=6,\n",
    "             stop={\"training_iteration\": 5000}\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1788ca6a-c617-4ab4-b20d-07573e897fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "env = gym.make('FetchPickAndPlace-v2', render_mode='rgb_array', reward_type='dense', max_episode_steps=100)\n",
    "#env = CustomWrapper(env)\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5c08a1-e3db-4898-abc5-96cea7fa15af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from ray.rllib.policy.policy import Policy     \n",
    "\n",
    "# Use the `from_checkpoint` utility of the Policy class:\n",
    "my_restored_policy = Policy.from_checkpoint(\"../R3L-LOGS/PPO/PPO_Gymnasium_Pick_Place_0_Dense_df6f8_00000_0_2023-07-28_10-19-37/checkpoint_004994/policies/default_policy\")\n",
    "\n",
    "# Use the restored policy for serving actions.\n",
    "obs,  reward, terminated, truncated, info =   env.step(env.action_space.sample())# individual CartPole observation\n",
    "#action = my_restored_policy.compute_single_action(np.append(obs.get('observation'), obs.get('achieved_goal'), obs.get('desired_goal')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5998a3af-0db1-4c43-aa25-fd47e8c50169",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 512):\n",
    "    action = my_restored_policy.compute_single_action(np.concatenate([obs.get('observation'), obs.get('desired_goal'), obs.get('achieved_goal')]))\n",
    "\n",
    "    obs,  reward, terminated, truncated, info =   env.step(env.action_space.sample())# individual CartPole observation\n",
    "\n",
    "    print(f\"Computed action {action} from given CartPole observation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298e7720-f15f-4f2a-b602-d379cb275275",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython import display as ipythondisplay\n",
    "img = plt.imshow(env.render())\n",
    "for i in range(0, 512):\n",
    "    \n",
    "    action = my_restored_policy.compute_single_action(np.concatenate([obs.get('observation'), obs.get('desired_goal'), obs.get('achieved_goal')]))\n",
    "\n",
    "    obs,  reward, terminated, truncated, info =   env.step(env.action_space.sample())# individual CartPole observation\n",
    "\n",
    "    #print(f\"Computed action {action} from given CartPole observation.\")\n",
    "    img.set_data(env.render()) # Just update the data\n",
    "    ipythondisplay.display(plt.gcf())\n",
    "    ipythondisplay.clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e05a194-db19-416e-9b63-de397b8093f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import ray\n",
    "import gymnasium as gym\n",
    "from ray.rllib.algorithms.ppo import PPO\n",
    "\n",
    "# Load the trained model.\n",
    "#ray.init()\n",
    "\n",
    "config = {\n",
    "    \"env\": \"Gymnasium_Pick_Place_0\", \n",
    "    # add any additional configuration settings here\n",
    "}\n",
    "agent = PPO(config=config)\n",
    "agent.restore(\"/home/cocp5/Gymnasium-Robotics-RL/Gymnasium_Pick_Place_0/PPO/PPO_Gymnasium_Pick_Place_0_d144a_00000_0_2023-07-25_15-52-03/checkpoint_000874\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcaf881f-a19c-4985-a302-9ad23848d961",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32aa85f7-7262-4f8b-a1c1-b92980c7f2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c2f713-aac4-4b68-9902-2016083af454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the environment\n",
    "env = gym.make('FetchPickAndPlace-v2', max_episode_steps=100)#(config[\"env\"])\n",
    "\n",
    "# Run the agent on the environment for 10 episodes.\n",
    "for i_episode in range(10):\n",
    "    observation, info = env.reset()\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "    while not done:\n",
    "        action = agent.compute_action(observation)\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "        episode_reward += reward\n",
    "        env.render()\n",
    "    print(f\"Episode {i_episode + 1} reward: {episode_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f35d8f-b1d3-4bd3-aaa3-c70087915dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import ray\n",
    "from ray.rllib.algorithms.ppo import PPOTrainer, DEFAULT_CONFIG\n",
    "\n",
    "# Initialize Ray.\n",
    "ray.init()\n",
    "\n",
    "# Define the configuration.\n",
    "config = DEFAULT_CONFIG.copy()\n",
    "config[\"num_gpus\"] = 0\n",
    "config[\"num_workers\"] = 1\n",
    "\n",
    "# Create the trainer.\n",
    "trainer = PPOTrainer(config=config, env=\"Gymnasium_Pick_Place_0\")\n",
    "\n",
    "# Load the trained model.\n",
    "trainer.restore(\"/path/to/the/saved/checkpoint\")\n",
    "\n",
    "# Run the agent on the environment for 10 episodes.\n",
    "for i_episode in range(10):\n",
    "    episode_reward = 0\n",
    "    done = False\n",
    "    obs = trainer.env.reset()\n",
    "    while not done:\n",
    "        action = trainer.compute_action(obs)\n",
    "        obs, reward, done, info = trainer.env.step(action)\n",
    "        episode_reward += reward\n",
    "        trainer.env.render()\n",
    "    print(f\"Episode {i_episode + 1} reward: {episode_reward}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
